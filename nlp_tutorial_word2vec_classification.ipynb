{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on using word2vec for classification and visualisation.\n",
    "Data included in repo classifies if a sentence is spam or ham.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the following \n",
    "import pandas as pd \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score,f1_score \n",
    "\n",
    "#Custom transformer for w2v\n",
    "import transformer as w2vt\n",
    "#Custom plotter\n",
    "import tsne as t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model and check output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data . CSV contains ham or spam emails and is labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "txts = pd.read_csv(\"./spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# remove dirty columns\n",
    "txts_clean = txts[[\"v1\", \"v2\"]]\n",
    "\n",
    "# rename the columns\n",
    "txts_clean = txts_clean.rename(columns={\"v1\": \"category\", \"v2\": \"message\"})\n",
    "\n",
    "# view\n",
    "txts_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test for classification of emails into spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Baseline is 0.8690191387559809\n"
     ]
    }
   ],
   "source": [
    "#y \n",
    "#recode y as the standard 0, 1\n",
    "y = txts_clean[\"category\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "#X\n",
    "X = txts_clean[\"message\"]\n",
    "#Clean text.\n",
    "X= X.str.replace('[^a-z ]', '').str.lower().str.split()\n",
    "\n",
    "#Split data into train and test. Leave imbalance in class labels \n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "#Check baseline\n",
    "print(f\" Baseline is {y_test.value_counts(normalize=True)[0]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run word2vec(CBOW) upsteam and adaboost classifier downsteam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy 0.9748803827751196\n",
      "Model f1_score 0.9743105322740244\n"
     ]
    }
   ],
   "source": [
    "#Word2vec Hyperparameters:\n",
    "#Dimensions=300, Window=5,Sample =.1, sg=0, hs=1,alpha=0.025, iterations/epoochs=20).\n",
    "\n",
    "#Define steps for pipeline\n",
    "#Gensim model\n",
    "gensim_word2vec_tr = w2vt.GensimWord2VecVectorizer(size=300,window=5, min_count=3,sample=.1, sg=0,hs=1, alpha=0.025, iter=20)\n",
    "#Classifier\n",
    "xgb = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
    " \n",
    "#Define pipeline \n",
    "w2v_xgb = Pipeline([('w2v', gensim_word2vec_tr),('xgb', xgb)])\n",
    "\n",
    "#Fit and score pipe\n",
    "w2v_xgb.fit(X_train,y_train)\n",
    "y_pred=w2v_xgb.predict(X_test)\n",
    "print(f\"Model accuracy {w2v_xgb.score(X_test,y_test)}\") \n",
    "print(f\"Model f1_score {f1_score(y_test,y_pred,average='weighted')}\") \n",
    "\n",
    "#Assign w2v model to variable for similary scores\n",
    "wv = w2v_xgb.named_steps['w2v'].model_.wv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test word2vec on word association task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('won', 0.7462610602378845),\n",
       " ('award', 0.7102481126785278),\n",
       " ('guaranteed', 0.7018029689788818),\n",
       " ('iod', 0.6873871684074402),\n",
       " ('rize', 0.6749557852745056),\n",
       " ('gift', 0.6169447302818298),\n",
       " ('player', 0.6068623661994934),\n",
       " ('fantastic', 0.5967285633087158),\n",
       " ('ideo', 0.5869741439819336),\n",
       " ('okia', 0.5858338475227356)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What words are most similar, return similar words with respective cosine.\n",
    "wv.most_similar(positive=['prize'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
